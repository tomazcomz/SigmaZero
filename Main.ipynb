{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project – Developing an Alpha Zero Game Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratory of Artificial Intelligence and Data Science\n",
    "\n",
    "#### TP1 - Group 7:\n",
    "##### Anna Sellani\n",
    "##### Gonçalo Dias\n",
    "##### Tomás Azevedo\n",
    "##### Vicente Bandeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MCTS import MCTS\n",
    "import tensorflow as tf\n",
    "import optimizar,avaliar,selfplay\n",
    "import Go,Attaxx\n",
    "from ioannina import Neura, get_best_name,exports,opts",
    "import socket\n",
    "import time\n",
    "from Go import GameState as Go, setScreen as set_screen_go, drawBoard as draw_board_go, drawPieces as draw_pieces_go, drawResult as draw_result_go\n",
    "from Attaxx import GameState as Attaxx, move as execute_move, _objective_test as is_game_finished_attaxx, get_moves, setScreen as set_screen_attaxx, drawBoard as draw_board_attaxx, drawPieces as draw_pieces_attaxx, drawResult as draw_result_attaxx\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [\"A4x4\", \"A5x5\", \"A6x6\", \"G7x7\", \"G9x9\"]\n",
    "game = games[0]     # ATTAXX\n",
    "game = games[3]     # GO\n",
    "\n",
    "INVALID_LIMIT = 2\n",
    "TIME_LIMIT = 10  # (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_move_valid_go(game: Go, move):    # implementing the logic to check if the move is valid\n",
    "    return move in game.check_possible_moves(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_server_go(host='localhost', port=12345):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(2)\n",
    "\n",
    "    print(\"Waiting for two agents to connect...\")\n",
    "    agent1, addr1 = server_socket.accept()\n",
    "    print(\"Agent 1 connected from\", addr1)\n",
    "    bs=b'AG1 '+game.encode()\n",
    "    agent1.sendall(bs)\n",
    "\n",
    "    agent2, addr2 = server_socket.accept()\n",
    "    print(\"Agent 2 connected from\", addr2)\n",
    "    bs=b'AG2 '+game.encode()\n",
    "    agent2.sendall(bs)\n",
    "       \n",
    "    n = int(game[1])\n",
    "    initial_board = np.zeros((n, n),dtype=int)     # initializing an empty board of size (n x n)\n",
    "    GameState = Go(initial_board)    # initializing the game\n",
    "    \n",
    "    pygame.init()\n",
    "    screen = set_screen_go()    # setting the screen for graphical display\n",
    "    draw_board_go(GameState, screen)\n",
    "    pygame.display.update()\n",
    "\n",
    "    agents = [agent1, agent2]\n",
    "    current_agent = 0\n",
    "\n",
    "    jog=0\n",
    "    invalid_count = 0   # consecutive invalid moves count \n",
    "    \n",
    "    time.sleep(3)\n",
    "    while True:\n",
    "        try:\n",
    "            data = None\n",
    "            data = agents[current_agent].recv(1024).decode()\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            if data == \"PASS\":\n",
    "                agents[current_agent].sendall(b'VALID')\n",
    "                agents[1-current_agent].sendall(data.encode())\n",
    "                GameState.pass_turn()\n",
    "            else:\n",
    "                # processing the move (example: \"MOVE X,Y\")\n",
    "                i = int(data[5])\n",
    "                j = int(data[7])\n",
    "                if current_agent == 0:\n",
    "                    print(\"Agent 1 -> \",data)\n",
    "                else:\n",
    "                    print(\"Agent 2 -> \",data)\n",
    "                jog = jog+1\n",
    "                \n",
    "                # checking if the move is valid and, if so, executing it\n",
    "                if is_move_valid_go(GameState,(i,j)):\n",
    "                    agents[current_agent].sendall(b'VALID')\n",
    "                    agents[1-current_agent].sendall(data.encode())\n",
    "                    GameState = GameState.move((i,j))\n",
    "                    time.sleep(0.1)\n",
    "                    draw_board_go(GameState, screen)\n",
    "                    draw_pieces_go(GameState, screen)\n",
    "                    event = pygame.event.poll()\n",
    "                else:\n",
    "                    agents[current_agent].sendall(b'INVALID')\n",
    "                    invalid_count += 1\n",
    "                    if invalid_count < INVALID_LIMIT:   # if invalid count reaches 5, then the agent passes\n",
    "                        continue\n",
    "                    agents[current_agent].sendall(b'TURN LOSS')\n",
    "                    agents[1-current_agent].sendall(b'PASS')\n",
    "                    GameState = GameState.pass_turn()\n",
    "                    invalid_count = 0\n",
    "                \n",
    "            pygame.display.update()\n",
    "                \n",
    "            # checking if the game is over\n",
    "            if GameState.is_game_finished():\n",
    "                GameState.end_game()\n",
    "                winner = GameState.winner\n",
    "                if winner == -1:\n",
    "                    winner = 2\n",
    "                p1_score = GameState.scores[1]\n",
    "                p2_score = GameState.scores[-1]\n",
    "                data = \"END \" + str(winner) + \" \" + str(p1_score) + \" \" + str(p2_score)\n",
    "                agents[current_agent].sendall(data.encode())\n",
    "                agents[1-current_agent].sendall(data.encode())\n",
    "                draw_result_go(GameState, screen)\n",
    "                pygame.display.update()\n",
    "                time.sleep(4)\n",
    "                pygame.quit()\n",
    "                break\n",
    "                \n",
    "            # Switch to the other agent\n",
    "            current_agent = 1-current_agent\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            break\n",
    "\n",
    "    print(\"\\n-----------------\\nGAME END\\n-----------------\\n\")\n",
    "    time.sleep(1)\n",
    "    agent1.close()\n",
    "    agent2.close()\n",
    "    server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_board_attaxx(n):\n",
    "    board = np.zeros((n, n),dtype=int)\n",
    "    board[0][0] = board[n-1][n-1] = 1\n",
    "    board[n-1][0] = board[0][n-1] = -1\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_move_valid_attaxx(GameState,i,j,i2,j2):\n",
    "    possible_moves = [(1,0),(2,0),(1,1),(2,2),(1,-1),(2,-2),(-1,0),(-2,0),(-1,1),(-2,-2),(0,1),(0,2),(0,-1),(0,-2),(-1,-1),(-2,2)]\n",
    "    move = (i2-i,j2-j)\n",
    "    if move not in possible_moves:\n",
    "        return False\n",
    "    moves = get_moves(GameState,(i,j))\n",
    "    return moves[move][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_server_attaxx(host='localhost', port=12345):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(2)\n",
    "\n",
    "    print(\"Waiting for two agents to connect...\")\n",
    "    agent1, addr1 = server_socket.accept()\n",
    "    print(\"Agent 1 connected from\", addr1)\n",
    "    bs=b'AG1 '+game.encode()\n",
    "    agent1.sendall(bs)\n",
    "\n",
    "    agent2, addr2 = server_socket.accept()\n",
    "    print(\"Agent 2 connected from\", addr2)\n",
    "    bs=b'AG2 '+game.encode()\n",
    "    agent2.sendall(bs)\n",
    "       \n",
    "    n = int(game[1])\n",
    "    initial_board = create_board_attaxx(n)     # initializing an empty board of size (n x n)\n",
    "    GameState = Attaxx(initial_board)    # initializing the game\n",
    "\n",
    "    pygame.init()\n",
    "    screen = set_screen_attaxx()    # setting the screen for graphical display\n",
    "    draw_board_attaxx(GameState, screen)\n",
    "    draw_pieces_attaxx(GameState, screen)\n",
    "    pygame.display.update()\n",
    "\n",
    "    agents = [agent1, agent2]\n",
    "    current_agent = 0\n",
    "    player_id = 1\n",
    "\n",
    "    jog=0\n",
    "    invalid_count = 0   # consecutive invalid moves count \n",
    "    \n",
    "    time.sleep(3)\n",
    "    while True:\n",
    "        try:\n",
    "            data = None\n",
    "            data = agents[current_agent].recv(1024).decode()\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            # processing the move (example: \"MOVE X,Y,X2,Y2\")\n",
    "            i = int(data[5])\n",
    "            j = int(data[7])\n",
    "            i2 = int(data[9])\n",
    "            j2 = int(data[11])\n",
    "            if current_agent == 0:\n",
    "                print(\"Agent 1 -> \",data)\n",
    "            else:\n",
    "                print(\"Agent 2 -> \",data)\n",
    "            jog = jog+1\n",
    "            \n",
    "            # checking if the move is valid and, if so, executing it\n",
    "            if is_move_valid_attaxx(GameState,i,j,i2,j2):\n",
    "                agents[current_agent].sendall(b'VALID')\n",
    "                agents[1-current_agent].sendall(data.encode())\n",
    "                GameState = execute_move(GameState,(i,j),(i2,j2),player_id=player_id)\n",
    "                time.sleep(0.1)\n",
    "                draw_board_attaxx(GameState, screen)\n",
    "                draw_pieces_attaxx(GameState, screen)\n",
    "                event = pygame.event.poll()\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "                if invalid_count < INVALID_LIMIT:   # if invalid count reaches 5, then the agent passes\n",
    "                    agents[current_agent].sendall(b'INVALID')\n",
    "                    continue\n",
    "                agents[current_agent].sendall(b'TURN LOSS')\n",
    "                agents[1-current_agent].sendall(b'PASS')\n",
    "                GameState.switchPlayer()\n",
    "                invalid_count = 0\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "            # checking if the game is over\n",
    "            value,score1,score2 = is_game_finished_attaxx(GameState,player=player_id)   # -1 if game is not over, 0 if it's a draw, 1 if player 1 won and 2 if player 2 won \n",
    "            if value != -2:\n",
    "                result = value\n",
    "                if result == 1:\n",
    "                    p1_score = score1\n",
    "                    p2_score = score2\n",
    "                else:\n",
    "                    p2_score = score1\n",
    "                    p1_score = score2\n",
    "                data = \"END \" + str(result) + \" \" + str(p1_score) + \" \" + str(p2_score)\n",
    "                agents[current_agent].sendall(data.encode())\n",
    "                agents[1-current_agent].sendall(data.encode())\n",
    "                draw_result_attaxx(GameState, screen)\n",
    "                pygame.display.update()\n",
    "                time.sleep(4)\n",
    "                pygame.quit()\n",
    "                break\n",
    "                \n",
    "            # Switch to the other agent\n",
    "            current_agent = 1-current_agent\n",
    "            player_id = 0-player_id\n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            break\n",
    "\n",
    "    print(\"\\n-----------------\\nGAME END\\n-----------------\\n\")\n",
    "    time.sleep(1)\n",
    "    agent1.close()\n",
    "    agent2.close()\n",
    "    server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if game[0]=='G':\n",
    "        start_server_go()\n",
    "    elif game[0]=='A':\n",
    "        start_server_attaxx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from go.inputconverter import *\n",
    "import time\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select, expand and evaluate, backup, play<br/>\n",
    "<br/>\n",
    "APV-MCTS variant <br/>\n",
    "<br/>\n",
    "N = visit_count<br/>\n",
    "W = total_action_value<br/>\n",
    "Q = mean_action_value<br/>\n",
    "P = prior_prob of selecting that edge<br/>\n",
    "exploration constant = cpuct <br/>\n",
    "<br/>\n",
    "Q = W/N # controlls exploitation<br/>\n",
    "U = cput*p*(math.sqrt(sum_N)/(1+N)) # controlls exploration<br/>\n",
    "<br/>\n",
    "edges (moves)<br/>\n",
    "nodes (positions/states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game_state, args, mcts, parent=None, p_action=None, prior_prob=0,play_idx=0):\n",
    "        self.game_state=game_state\n",
    "        self.args=args\n",
    "        self.parent=parent\n",
    "        self.p_action=p_action\n",
    "        self.prior_prob=prior_prob  # P\n",
    "        self.children=[]\n",
    "        self.visit_count=0  # N\n",
    "        self.total_action_value=0   # W\n",
    "        self.possible=self.game_state.n**2+self.game_state.type\n",
    "        self.mcts=mcts\n",
    "        self.play_idx=play_idx\n",
    "\n",
    "    def fully_expanded(self):\n",
    "        return len(self.children)>0     # if no expandable moves and there are children\n",
    "    \n",
    "    def select(self):   # chooses child with best ucb \n",
    "        if not self.fully_expanded():\n",
    "            return self\n",
    "        selected = max(self.children, key=lambda child: self.ucb(child))\n",
    "        return selected.select()\n",
    "    \n",
    "    def cpuct(self, visit_count):   # defining cpuct according to paper\n",
    "        return math.log((visit_count+19652+1)/19652)+self.args['cpuct']\n",
    "    \n",
    "    def ucb(self, child):   # uses variant of the PUCT algorithm\n",
    "        if child is None:       # to avoid 'NoneType' error\n",
    "            return 0\n",
    "        if child.visit_count==0:\n",
    "            mean_action_value=0     \n",
    "        else:\n",
    "            mean_action_value=child.total_action_value/child.visit_count        # mean_action_value Q=W/N\n",
    "        return mean_action_value+self.cpuct(self.visit_count)*child.prior_prob*(math.sqrt(self.visit_count)/(1+child.visit_count))\n",
    "\n",
    "    def expand(self, p):\n",
    "        for _ in range(self.possible):\n",
    "            action=self.mcts.get_act(_)\n",
    "            if action in self.game_state.empty_positions or action==(-1,-1):    # to avoid 'NoneType' error\n",
    "                next_state = self.game_state.move(action)\n",
    "                child = Node(next_state,self.args, parent=self, p_action=action, prior_prob=p[_],mcts=self.mcts,play_idx=self.play_idx+1)\n",
    "                self.children.append(child)\n",
    "    \n",
    "    def backprop(self, v):\n",
    "        self.total_action_value  += v\n",
    "        self.visit_count += 1\n",
    "        if self.parent is not None:\n",
    "            self.parent.backprop(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, game_state, args, model,eva=False):\n",
    "        self.game_state=game_state\n",
    "        self.args=args\n",
    "        self.model=model\n",
    "        self.evaluate=eva\n",
    "        self.ti=self.setind(game_state)\n",
    "        self.root=Node(self.game_state, self.args,self)\n",
    "        self.pi=np.zeros(self.game_state.n**2+self.game_state.type)\n",
    "        self.map=self.map_act()\n",
    "\n",
    "    def setind(self,game):  # temperature according to game and boards\n",
    "        if game.type==0:\n",
    "            match len(game.board):\n",
    "                case 4:\n",
    "                    tind=2\n",
    "                case 6:\n",
    "                    tind=3\n",
    "        else:\n",
    "            match len(game.board):\n",
    "                case 7:\n",
    "                    tind=5\n",
    "                case 9:\n",
    "                    tind=7\n",
    "        return tind\n",
    "    \n",
    "    def map_act(self):  # mapping actions\n",
    "        list=[]\n",
    "        if self.game_state.name=='attaxx':\n",
    "            for i in range(len(self.game_state.board)):\n",
    "                for j in range(len(self.game_state.board[0])):\n",
    "                    list.append((j,i))\n",
    "        else:\n",
    "            for i in range(len(self.game_state.board)):\n",
    "                for j in range(len(self.game_state.board[0])):\n",
    "                    list.append((i,j))\n",
    "            list.append((-1,-1))\n",
    "        return list\n",
    "\n",
    "    def get_act(self,_):\n",
    "        return  self.map[_]\n",
    "    \n",
    "    def cut(self,action):   # new root node is the child corresponding to the played action\n",
    "        for child in self.root.children:\n",
    "            if child.p_action==action:\n",
    "                self.root=child\n",
    "                self.pi=np.zeros(self.game_state.n**2+self.game_state.type)\n",
    "\n",
    "    def printTree(self, node, level=0, prefix=\"\"):  # analysis purposes only\n",
    "        if node is not None:\n",
    "            print(\" \" * level * 2 + f\"{prefix}+- action: {node.p_action}, N: {node.visit_count}, W: {node.total_action_value}\")\n",
    "            for i, child in enumerate(node.children):\n",
    "                self.printTree(child, level + 1, f\"{prefix}|  \" if i < len(node.children) - 1 else f\"{prefix}   \")\n",
    "\n",
    "    def get_play(self,passe=None):  # chooses move based on maximum pi\n",
    "        max_val=0\n",
    "        ind=[]\n",
    "        for i in range(len(self.pi)):\n",
    "            if i==passe:\n",
    "                continue\n",
    "            val=self.pi[i]\n",
    "            if val>max_val:\n",
    "                ind=[i]\n",
    "                max_val=self.pi[i]\n",
    "                continue\n",
    "            if val==max_val:\n",
    "                ind.append(i)\n",
    "        return random.choice(ind)\n",
    "    \n",
    "    def play(self):\n",
    "        for _ in range(self.args['num_searches']):\n",
    "            node=self.root\n",
    "\n",
    "            # selection\n",
    "            while node.fully_expanded():\n",
    "                node=node.select()\n",
    "\n",
    "            # check if node is terminal or not\n",
    "            terminal=self.game_state.is_game_finished()\n",
    "            \n",
    "            # expand and evaluate\n",
    "            if not terminal:\n",
    "                if self.game_state.type==1:\n",
    "                    board=gen_batch(node.game_state)\n",
    "                else:\n",
    "                    board=node.game_state.board\n",
    "                p, v = self.model.net.predict(np.array([board]),batch_size=1,verbose=0)\n",
    "                p=p[0]\n",
    "                v=v[0][0]\n",
    "                if self.root.play_idx-1>self.ti or self.evaluate:\n",
    "                    p=0.75*p+0.25*np.random.dirichlet([0.2,0.2,0.2])[0]     # adding Dirichlet noise to root's prior \n",
    "\n",
    "                node.expand(p)      # adding children with policy from the NN to list children\n",
    "            \n",
    "            # backpropagate\n",
    "            node.backprop(v)\n",
    "\n",
    "        if self.root.play_idx-1<=self.ti and not self.evaluate:\n",
    "            temp=1\n",
    "        else:\n",
    "            temp=10**(-2)\n",
    "\n",
    "        sumb=decimal.Decimal(0)\n",
    "        for child in self.root.children:\n",
    "            if child is None or child.visit_count==0:\n",
    "                continue\n",
    "            else:\n",
    "                sumb+=(decimal.Decimal(child.visit_count)**decimal.Decimal(1/temp))\n",
    "                    \n",
    "        for child in self.root.children:\n",
    "            if child is None:\n",
    "                continue\n",
    "            if child.visit_count == 0:\n",
    "                self.pi[self.map.index(child.p_action)] = 0\n",
    "            elif child.visit_count == 1:\n",
    "                self.pi[self.map.index(child.p_action)] = 0.1\n",
    "            else: \n",
    "                self.pi[self.map.index(child.p_action)] = (float)((decimal.Decimal(child.visit_count)**decimal.Decimal((1/temp)))/(sumb))\n",
    "        \n",
    "        pol=self.pi\n",
    "        max_prob_index=self.get_play()\n",
    "        if max_prob_index == self.game_state.n**2:\n",
    "            self.cut((-1,-1))\n",
    "            return (-1, -1),pol     # define this as \"pass\"\n",
    "        else:\n",
    "            played=((max_prob_index // self.game_state.n), (max_prob_index % self.game_state.n))    # convert 1D array index to 2D array coordinates\n",
    "            print(f\"Play chosen: {played}\")     # analysis purposes only\n",
    "            self.cut(played) \n",
    "            return played,pol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following folllows an adaptation of the original AlphaZero proposed by He, K. et Al. (2016).\n",
    "\n",
    "Main changes follow:\n",
    "\n",
    "    * number of filters.\n",
    "    * attaxx input is 2D.\n",
    "\n",
    "Input for Go is the proposed t,...,t-7 original approach and an 8-padded Attaxx board for size flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers,regularizers,optimizers\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "import os, random\n",
    "import names\n",
    "from go.inputconverter import *\n",
    "from shutil import copy\n",
    "import math\n",
    "from avaliar import makegame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neura:\n",
    "    def __init__(self,game,name=None): \n",
    "        self.input(game)\n",
    "        self.game=game\n",
    "        self.res=19\n",
    "        self.build(self.res,self.nf)\n",
    "        if name=='acacio':                          # for alternative approach testing\n",
    "            self.name='acacio'+game.name+str(len(game.board))+str(self.res)\n",
    "        elif name==None:\n",
    "            self.name=names.get_last_name()+game.name+str(len(game.board))+str(self.res)\n",
    "            self.net.save_weights(f'modelos/{game.name}/{str(len(game.board))}/{self.name}.h5')\n",
    "        else:\n",
    "            self.name=name\n",
    "            self.net.load_weights(f'modelos/{game.name}/{str(len(game.board))}/{self.name}.h5')\n",
    "        \n",
    "\n",
    "    def input(self,game):\n",
    "        if (game.type==0):\n",
    "            self.nf=math.ceil(len(game.board)**2*0.709+len(game.board)**2)                 \n",
    "            self.inpt=layers.Input(shape=(len(game.board),len(game.board[0]),1))\n",
    "            self.passes=0\n",
    "        else:\n",
    "            self.nf=math.ceil(len(game.board)**2*0.709+len(game.board)**2)\n",
    "            self.passes=1\n",
    "            self.inpt=layers.Input((len(game.board),len(game.board[0]),17))\n",
    "        self.action_space=len(game.board)*len(game.board[0])+self.passes\n",
    "\n",
    "    def convblock(self,input,nf):\n",
    "        c=layers.Conv2D(nf,3,(1,1),'same',kernel_regularizer=regularizers.L2(0.0001))(input)\n",
    "        b=layers.BatchNormalization()(c)\n",
    "        rnl=layers.Activation(activation='softplus')(b)\n",
    "        return rnl\n",
    "\n",
    "    def resblock(self,input,i,nf):\n",
    "        cb=self.convblock(input,nf)\n",
    "        c=layers.Conv2D(nf,3,(1,1),'same',kernel_regularizer=regularizers.L2(0.0001))(cb)\n",
    "        b=layers.BatchNormalization()(c)\n",
    "        s=layers.Add()([b,input])\n",
    "        rnl=layers.Activation(activation='softplus',name=f'endrestower{i}')(s)\n",
    "        return rnl\n",
    "    \n",
    "    def polhead(self,input):\n",
    "        c=layers.Conv2D(2,1,(1,1),'same',name='convpol',kernel_regularizer=regularizers.L2(0.0001))(input)\n",
    "        b=layers.BatchNormalization(name='bnpol')(c)\n",
    "        rnl=layers.Activation(activation='softplus',name='rnlpol')(b)\n",
    "        flt=layers.Flatten(name='polflat')(rnl)\n",
    "        fc=layers.Dense(units=self.action_space,name='polout',kernel_regularizer=regularizers.L2(0.0001))(flt)  \n",
    "        return fc\n",
    "\n",
    "    def valhead(self,input,nf):\n",
    "        c=layers.Conv2D(1,1,(1,1),'same',kernel_regularizer=regularizers.L2(0.0001))(input)\n",
    "        b=layers.BatchNormalization()(c)\n",
    "        rnl=layers.Activation(activation='softplus')(b)\n",
    "        flt=layers.Flatten()(rnl)\n",
    "        fcl=layers.Dense(nf,kernel_regularizer=regularizers.L2(0.0001))(flt)\n",
    "        rnl2=layers.Activation(activation='softplus')(fcl)\n",
    "        fcs=layers.Dense(1,kernel_regularizer=regularizers.L2(0.0001))(rnl2)\n",
    "        tanh=layers.Activation(activation='tanh',name='valout')(fcs)\n",
    "        return tanh    \n",
    "    \n",
    "    def build(self,n_res,nf,):\n",
    "        conv=self.convblock(self.inpt,nf)\n",
    "        restower=conv\n",
    "        for i in range(n_res):\n",
    "            restower=self.resblock(restower,i,nf)\n",
    "        polh=self.polhead(restower)\n",
    "        valh=self.valhead(restower,nf)\n",
    "        outputs=[polh,valh]\n",
    "        self.net=Model(self.inpt,outputs)\n",
    "        return\n",
    "\n",
    "    def summary(self):\n",
    "        self.net.summary()\n",
    "        return\n",
    "    \n",
    "    def compilar(self,lr=0.01):\n",
    "        self.net.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.9),loss={'polout':tf.keras.losses.CategoricalCrossentropy(),'valout':tf.keras.losses.MeanSquaredError()})\n",
    "\n",
    "\n",
    "    # Funções logísticas para armazenamento e substituição de modelos\n",
    "    def copy_weights(self,bestname):\n",
    "        src=f'modelos/{self.game.name}/{str(len(self.game.board))}/best/{bestname}.h5'\n",
    "        dest=f'modelos/{self.game.name}/{str(len(self.game.board))}/{bestname}.h5'\n",
    "        copy(src, dest)\n",
    "    \n",
    "    def make_best(self):\n",
    "        os.remove((f'modelos/{self.game.name}/{str(len(self.game.board))}/best/{self.get_best_name()}.h5'))\n",
    "        self.net.save_weights(f'modelos/{self.game.name}/{str(len(self.game.board))}/best/{self.name}.h5')\n",
    "\n",
    "    def get_best_name(self):\n",
    "        folder_path = f\"modelos/{self.game.name}/{len(self.game.board)}/best\"\n",
    "        entries = os.listdir(folder_path)\n",
    "        for e in entries:\n",
    "            file_name = e\n",
    "            break\n",
    "        return file_name[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_name(game):\n",
    "    folder_path = f\"modelos/{game.name}/{len(game.board)}/best\"\n",
    "    entries = os.listdir(folder_path)\n",
    "    file_name = None\n",
    "    for e in entries:\n",
    "        file_name = e\n",
    "        break\n",
    "    return file_name[:-3]\n",
    "\n",
    "\n",
    "# Loss Function, igual à proposta no artigo original, em que o parâmetro de regularização está aplicado nas Layers e não explicitamente na Loss\n",
    "def sigmaloss(y_true,y_pred):\n",
    "        pol=y_pred[0]\n",
    "        pit=np.transpose(np.array(y_true[0]))\n",
    "        return (y_pred[1]-y_true[1])**2-np.dot(pit,np.log(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game=makegame('G7x7')\n",
    "model=Neura(game,f'best/{get_best_name(game)}')\n",
    "model.compilar()\n",
    "\n",
    "# São necessárias dependências para projetar a rede, em alternativa poderá ser usado o método model.summary()\n",
    "\n",
    "#tf.keras.utils.plot_model(model.net, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communications protocol of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move_go():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move_attaxx():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move(game_name):   # returns the move in the form \"MOVE X,Y\"\n",
    "    if game_name=='go':\n",
    "        return choose_move_go()\n",
    "    else:\n",
    "        return choose_move_attaxx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_server(host='localhost', port=12345):\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect((host, port))\n",
    "\n",
    "    response = client_socket.recv(1024).decode()\n",
    "    print(f\"Server ResponseINIT: {response}\")\n",
    "\n",
    "    Game = response[-4:]\n",
    "    print(\"Playing:\", Game)\n",
    "    if Game[0]=='A':\n",
    "        game_name = 'attaxx'\n",
    "    else:\n",
    "        game_name = 'go'\n",
    "    n = int(Game[1])\n",
    "\n",
    "    if \"1\" in response:\n",
    "        ag=1\n",
    "    else:\n",
    "        ag=2\n",
    "    first=True\n",
    "\n",
    "    game_state=avaliar.makegame(Game)\n",
    "    teta=Neura(game_state,'Eakesgo7')\n",
    "    alpha=MCTS(game_state,ARGS,teta)\n",
    "\n",
    "    while True:\n",
    "        # Generate and send a random move\n",
    "        if ag == 1 or not first:\n",
    "            move = alpha.play()\n",
    "            time.sleep(1)\n",
    "            smove=str(move)\n",
    "            client_socket.send(smove.encode())\n",
    "            print(\"Send:\",move)\n",
    "        \n",
    "            # Wait for server response\n",
    "            response = client_socket.recv(1024).decode()\n",
    "            print(f\"Server Response1: {response}\")\n",
    "            if response == \"INVALID\":\n",
    "                continue\n",
    "            if \"END\" in response: break\n",
    "            game_state=game_state.move(move)\n",
    "            \n",
    "        first=False\n",
    "        response = client_socket.recv(1024).decode()\n",
    "        if response == \"PASS\":\n",
    "            game_state = game_state.pass_turn()\n",
    "        else:\n",
    "            i=response[5]\n",
    "            j=response[7]\n",
    "            if game_name == \"attaxx\":\n",
    "                i2=response[9]\n",
    "                j2=response[11]\n",
    "        action=(int(i),int(j))\n",
    "        print(f\"Server Response2: {response}\")\n",
    "        if \"END\" in response: break\n",
    "        game_state=game_state.move(action)\n",
    "        alpha.cut(action)\n",
    "\n",
    "    client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    connect_to_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no results to show since no model was built successfully due to incompletness of the selfplay phase. Any contrary indications found anywhere else in this project are the result of group misscommunication. We can only tell that the flexibility feature is working as expected. Nothing else to report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go9x9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the implications of computational power required no further development was carried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go7x7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a description of empirical analysis of the training process:\n",
    "\n",
    "    The first trained model learned fairly and was subsequently replaced by the next checkpoint. From here, no other checkpoint could beat this one. This model learned a strategy to corner the bottom line*. 'Howardgo719'\n",
    "    In an attempt to surpass this stranded model. Additional regularization was introduced and weights were razed and optimized again. This model beat the previous best and showed significant improvement but did not surpass the evaluation threshold. (Its weights were overwritten by the new fixed model, which hasn't learned properly yet, unfortunately)\n",
    "    Due to an error that was discovered in the final hours of the project in the MCTS algorithm that rendered all the past optimization, data generation and evaluation meaningless. We are hopeful this correction enables future training to be efficient and the future modules to be increasingly better. We haven't had time to test it yet.\n",
    "\n",
    "    * to check for this, one has to rollback the error in the MCTS (making the policy formula to the wrong (self.root.visit_count/child.visit_count)**(1/t) for child in root.children) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,   \n",
    "  Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel & Demis Hassabis. (2017, October 19).   \n",
    "    **Mastering the game of Go without human knowledge**. doi:10.1038/nature24270\n",
    "\n",
    "- Johannes Czech, Patrick Korus & Kristian Kersting. (2020, December 22)\n",
    "    **Monte-Carlo Graph Search for AlphaZero**. arXiv:2012.11045v1 [cs.AI] \n",
    "\n",
    "- He, K., Zhang, X., Ren, S. & Sun, J. **Deep residual learning for image recognition**.\n",
    "  In Proc. 29th IEEE Conf. Comput. Vis. Pattern Recognit. 770–778 (2016)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
